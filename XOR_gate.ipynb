{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "tf.set_random_seed(55)\n",
    "np.random.seed(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FNN(object):\n",
    "    \"\"\"Build a general FeedForward neural network\n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate: float\n",
    "    drop_out: float\n",
    "    Layers: list\n",
    "    N_hidden: list\n",
    "    D_input: int\n",
    "    D_label: int\n",
    "    Task_type: string\n",
    "        'regression' or 'classification'\n",
    "    L2_lambda: float\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate, drop_out, Layers, N_hidden,\n",
    "                 D_input, D_label, Task_type='regression', L2_lambda=0.0):\n",
    "        #var\n",
    "        self.learning_rate = learning_rate\n",
    "        self.drop_out = drop_out\n",
    "        self.Layers = Layers\n",
    "        self.N_hidden = N_hidden\n",
    "        self.D_input = D_input\n",
    "        self.D_label = D_label\n",
    "        #类型控制loss函数的选择\n",
    "        self.Task_type = Task_type\n",
    "        #L2 regularization的惩罚强弱，过高会使输出倾向于0\n",
    "        self.L2_lambda = L2_lambda\n",
    "        #用于存放锁累积的每层L2 regularization\n",
    "        self.l2_penalty = tf.constant(0.0)\n",
    "        \n",
    "        #用于生成缩放图，括号里起名字\n",
    "        with tf.name_scope('Input'):\n",
    "            self.inputs = tf.placeholder(tf.float32,[None,D_input],name='inputs')\n",
    "        with tf.name_scope('Label'):\n",
    "            self.labels = tf.placeholder(tf.float32,[None,D_label],name='labels')\n",
    "        with tf.name_scope('keep_rate'):\n",
    "            self.drop_keep_rate = tf.placeholder(tf.float32,name='dropout_keep')\n",
    "        \n",
    "        #初始化时直接生成，build方法是后面会建立的\n",
    "        self.build('F')\n",
    "        \n",
    "    def weight_init(self,shape):\n",
    "        #shape: list [in_dim, out_dim]\n",
    "        #can change initialization here\n",
    "        initial = tf.random_uniform(shape,minval=-np.sqrt(5)*np.sqrt(1.0/shape[0]),maxval=np.sqrt(5)*np.sqrt(1.0/shape[0]))\n",
    "        return tr.Variable(initial)\n",
    "    \n",
    "    def bias_init(self, shape):\n",
    "        #can change initialization here\n",
    "        initial = tf.constant(0.1,shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def varialbe_summaries(self, var, name):\n",
    "        with tf.name_scope(name+'_summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.scalar_summary('mean/'+name, mean)\n",
    "        with tf.name_scope(name+'_stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        #record the change data after each training\n",
    "        tf.scalar_summary('_stddev/'+name,stddev)\n",
    "        tf.scalar_summary('_max/'+name,tf.reduce_max(var))\n",
    "        tf.scalar_summary('_min/'+name,tf.reduce_min(var))\n",
    "        tf.histogram_summary(name,var)\n",
    "    \n",
    "    def layer(self, in_tensor, in_dim, out_dim, layer_name, act=tf.nn.relu):\n",
    "        with tf.name_scope(layer_name):\n",
    "            with tf.name_scope(layer_name+'_weights'):\n",
    "                # initial by function weight_init()\n",
    "                weights = self.weight_init([in_dim, out_dim])\n",
    "                # 存放每一个权重W\n",
    "                self.W.append(weights)\n",
    "                # 对权重进行统计\n",
    "                self.variable_summaries(weights, layer_name+'/weights')\n",
    "            with tf.name_scope(layer_name+'_biases'):\n",
    "                biases = self.bias_init([out_dim])\n",
    "                self.variable_summaries(biases, layer_name+'/biases')\n",
    "            with tf.name_scope(layer_name+'_Wx_plus_b'):\n",
    "                #calculate Wx+b\n",
    "                pre_activate = tf.matmul(in_tensor, weights) + biases\n",
    "                #记录直方图\n",
    "                tf.histogram_summary(layer_name+'/pre_activations',pre_activate)\n",
    "                \n",
    "            #计算a(Wx+b)\n",
    "            activations = act(pre_activate, name='activation')\n",
    "            tf.histogram_summary(layer_name+'/activations',activations)\n",
    "        #返回该层的输出以及权重W的l2\n",
    "        return activations, tf.nn.l2_loss(weights)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
